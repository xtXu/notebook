# 数值优化——牛顿法

## 牛顿法
牛顿法的基本思路为，利用原函数在 $\boldsymbol{x}$ 处的二阶泰勒展开来近似：
$$
f(\boldsymbol{x}) \approx \hat{f}(\boldsymbol{x}) \triangleq f\left(\boldsymbol{x}_k\right)+\nabla f\left(\boldsymbol{x}_k\right)^T\left(\boldsymbol{x}-\boldsymbol{x}_k\right)+\frac{1}{2}\left(\boldsymbol{x}-\boldsymbol{x}_k\right)^T \nabla^2 f\left(\boldsymbol{x}_k\right)\left(\boldsymbol{x}-\boldsymbol{x}_k\right)
$$
然后，求这个近似的二次函数的极小值点，
$$
\begin{aligned}
& \nabla \hat{f}(\boldsymbol{x})=\nabla^2 f\left(\boldsymbol{x}_k\right)\left(\boldsymbol{x}-\boldsymbol{x}_k\right)+\nabla f\left(\boldsymbol{x}_k\right)=\mathbf{0} \\
& \Longrightarrow \boldsymbol{x}=\boldsymbol{x}_k-\left[\nabla^2 f\left(\boldsymbol{x}_k\right)\right]^{-1} \nabla f\left(\boldsymbol{x}_k\right)
\end{aligned}
$$
为了保证求得的为极小而非极大，需要海森矩阵正定(PD)： $\nabla^2 f\left(\boldsymbol{x}_k\right) \succ \boldsymbol{O}$。  
将该极小值点作为下次迭代的起点，则可得**牛顿迭代公式**：
$$
\boldsymbol{x}_{k+1}=\boldsymbol{x}_k-\left[\nabla^2 f\left(\boldsymbol{x}_k\right)\right]^{-1} \nabla f\left(\boldsymbol{x}_k\right)
$$
![](../Resources/damped_newton_method_img_1.png)

我们主要从三方面来衡量一个数值优化方法：
1. 收敛速度
2. 稳定性
3. 每次迭代的计算量

**对于牛顿法，虽然迭代次数很少，但每次迭代都要求海森矩阵的逆，因此计算量大，计算效率低。**  
**另外，迭代时需要保证每一步的海森矩阵正定，但实际难以保证。**
![](../Resources/damped_newton_method_img_3.png)

## 实用牛顿法

常用的牛顿法流程如下:
![](../Resources/damped_newton_method_img_4.png)

为了求得 $x$ 的变化 $\boldsymbol{d}$，需要求矩阵 $\boldsymbol{M}$ 的逆，但求逆操作十分耗时。因此，为了提高计算效率，直接求解线性方程组 $\boldsymbol{Md}=\nabla f(\boldsymbol{x})$，即
$$
\left[\nabla^2 f(\boldsymbol{x})\right] \boldsymbol{d}=-\nabla f(\boldsymbol{x})
$$
另外，当海森阵为半正定(PSD)或不定时，问题的条件很差。此时，为了保证矩阵正定，我们选择一个与海森阵相近的矩阵 $\boldsymbol{M}$:
+ 当函数 $f(x)$ 为**凸函数**时，海森阵一定半正定，此时$$\boldsymbol{M}=\nabla^2 f(\boldsymbol{x})+\epsilon \boldsymbol{I}, \epsilon=\min \left(1,\|\nabla f(\boldsymbol{x})\|_{\infty}\right) / 10$$
  一定正定。（证明见后）  
  然后，可以通过 Cholesky 分解来求解线性方程组，从而得到 $\boldsymbol{d}$：
  $$\boldsymbol{M} \boldsymbol{d}=-\nabla f(\boldsymbol{x}), \boldsymbol{M}=\boldsymbol{L} \boldsymbol{L}^{\mathrm{T}}$$
+ 当函数 $f(x)$ **非凸**时，海森阵不定。因此，我们通过
+ If the function is **convex**, the Hessian must be PSD, then$$\boldsymbol{M}=\nabla^2 f(\boldsymbol{x})+\epsilon \boldsymbol{I}, \epsilon=\min \left(1,\|\nabla f(\boldsymbol{x})\|_{\infty}\right) / 10$$ $\boldsymbol{M}$ is PD, since the $\nabla^2 f(\boldsymbol{x})$ is PSD, then $\forall \boldsymbol{x}\neq 0$, $\boldsymbol{x}^\top \nabla^2 f(\boldsymbol{x}) \boldsymbol{x}\geq 0$, $$\boldsymbol{x}^\top\boldsymbol{M}\boldsymbol{x}=\boldsymbol{x}^\top(\nabla^2 f(\boldsymbol{x})+\epsilon\boldsymbol{I})\boldsymbol{x}=\boldsymbol{x}^\top \nabla^2 f(\boldsymbol{x}) \boldsymbol{x}+\boldsymbol{x}^\top \epsilon\boldsymbol{I}\boldsymbol{x}> 0$$ Then the search direction is solved by Cholesky factorization$$\boldsymbol{M} \boldsymbol{d}=-\nabla f(\boldsymbol{x}), \boldsymbol{M}=\boldsymbol{L} \boldsymbol{L}^{\mathrm{T}}$$
+ If the function is **nonconvex**, the Hessian is indefinite. We compute $\boldsymbol{M}$ through Bunch-Kaufman Factorization$$\boldsymbol{M d}=-\nabla f(\boldsymbol{x}), \boldsymbol{M}=\boldsymbol{L} \boldsymbol{B} \boldsymbol{L}^{\mathrm{T}}$$ where $\boldsymbol{B}$ is block diagonal matrix $[b_1, b_2, \cdots, b_n]$ with block size $1\times 1$ and $2\times 2$. $b_i \in R^+ \geq 0$, or $b_i\in R^{2\times 2}$ has one negative and one positive eigenvalue.  